{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어 공부.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXO78uNihxbWjbPjO55CTU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Choigapju/NLP-study/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4_%EA%B3%B5%EB%B6%80_%EB%AC%B8%EC%9E%90%EB%8B%A8%EC%9C%84RNN(Char%20RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "F3GmQjZtJU8b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewl_BdQgJwGc",
        "outputId": "571728f4-c0e9-48e1-fbab-4eb2c557db53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('11-0.txt', <http.client.HTTPMessage at 0x7f1e41537590>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('11-0.txt', 'rb')\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for sentence in f: # 데이터로부터 한 줄씩 읽는다.\n",
        "  sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거.\n",
        "  sentence = sentence.lower() # 소문자화.\n",
        "  sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
        "\n",
        "  if len(sentence) > 0:\n",
        "    sentences.append(sentence)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "yfSX4snzKIMD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPumDUznKvsG",
        "outputId": "9ff376f2-9b7f-4df3-b54e-6f6c6e3a01f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
              " 'this ebook is for the use of anyone anywhere in the united states and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
              " 'of the project gutenberg license included with this ebook or online at']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = ' '.join(sentences)\n",
        "print('문자열의 길이 또는 총 문자의 개수: %d' % len(total_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGnshRCYK8rp",
        "outputId": "14d37343-f132-47a2-b6bc-23afb13a457b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자열의 길이 또는 총 문자의 개수: 159484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSv8oDCLL9w",
        "outputId": "7a35bf9c-e6f1-4880-cea8-07b2c5329043"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = sorted(list(set(total_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5dHB4mqLPA9",
        "outputId": "a338df9f-2198-4289-e642-544444c33abe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자에 고유한 정수 부여\n",
        "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
        "print('문자 집합 : ', char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx8QlUVmLc9_",
        "outputId": "6af45191-0d3e-4b2f-e4e2-765554ae6512"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 :  {' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char = {}\n",
        "for key, value in char_to_index.items():\n",
        "  index_to_char[value] = key"
      ],
      "metadata": {
        "id": "lGDfDYQNLxzC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# appl (입력 시퀀스) -> pple (예측해야 하는 시퀀스)\n",
        "train_X = 'appl'\n",
        "train_y = 'pple'"
      ],
      "metadata": {
        "id": "R-5SNzJFL-bm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 60\n",
        "\n",
        "#문자열의 길이를 seq_length로 나누면 전처리 후 생겨날 샘플 수\n",
        "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
        "print('샘플의 수 : {}'.format(n_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_AUPyb-MOB6",
        "outputId": "82e2a650-e82d-4dbe-afab-095a412fec93"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 수 : 2658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "  # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 pick\n",
        "  X_sample = total_data[i * seq_length: (i+1)*seq_length]\n",
        "\n",
        "  #정수 인코딩\n",
        "  X_encoded = [char_to_index[c] for c in X_sample]\n",
        "  train_X.append(X_encoded)\n",
        "\n",
        "  #오른쪽으로 1칸 쉬프트\n",
        "  y_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\n",
        "  y_encoded = [char_to_index[c] for c in y_sample]\n",
        "  train_y.append(y_encoded)"
      ],
      "metadata": {
        "id": "15T2huwdMjfY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X 데이터의 첫번째 샘플 : ', train_X[0])\n",
        "print('y 데이터의 첫번째 샘플 : ', train_y[0])\n",
        "print('-'*50)\n",
        "print('X 데이터의 첫번째 샘플 디코딩 : ', [index_to_char[i] for i in train_X[0]])\n",
        "print('y 데이터의 첫번째 샘플 디코딩 : ', [index_to_char[i] for i in train_y[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GigpsescNZTo",
        "outputId": "a1b95168-5905-4998-e8f5-cf0aada81cfe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X 데이터의 첫번째 샘플 :  [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\n",
            "y 데이터의 첫번째 샘플 :  [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\n",
            "--------------------------------------------------\n",
            "X 데이터의 첫번째 샘플 디코딩 :  ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\n",
            "y 데이터의 첫번째 샘플 디코딩 :  ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztc-sCqyN_PJ",
        "outputId": "c645b78b-a072-4241-93cb-5b6db2be7fbf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "324j7oTuPFdA",
        "outputId": "a4153d0f-e8db-4480-b8f8-ec04c148b407"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = to_categorical(train_X)\n",
        "train_y = to_categorical(train_y)\n",
        "\n",
        "print('train_X의 크기(shape) : {}'.format(train_X.shape))\n",
        "print('train_y의 크기(shape) : {}'.format(train_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yULpklh7PHui",
        "outputId": "73ac98f2-80d7-4d27-9a45-afbc33d353f4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X의 크기(shape) : (2658, 60, 56)\n",
            "train_y의 크기(shape) : (2658, 60, 56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 설계하기\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
        "\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(hidden_units, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_X, train_y, epochs=80, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4J94gL6PcOh",
        "outputId": "5a380a23-a444-4f70-9438-8d51485f9119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "84/84 - 46s - loss: 3.0841 - accuracy: 0.1788 - 46s/epoch - 546ms/step\n",
            "Epoch 2/80\n",
            "84/84 - 41s - loss: 2.7560 - accuracy: 0.2457 - 41s/epoch - 490ms/step\n",
            "Epoch 3/80\n",
            "84/84 - 40s - loss: 2.3869 - accuracy: 0.3291 - 40s/epoch - 470ms/step\n",
            "Epoch 4/80\n",
            "84/84 - 40s - loss: 2.2371 - accuracy: 0.3639 - 40s/epoch - 478ms/step\n",
            "Epoch 5/80\n",
            "84/84 - 39s - loss: 2.1293 - accuracy: 0.3916 - 39s/epoch - 468ms/step\n",
            "Epoch 6/80\n",
            "84/84 - 41s - loss: 2.0376 - accuracy: 0.4142 - 41s/epoch - 489ms/step\n",
            "Epoch 7/80\n",
            "84/84 - 41s - loss: 1.9616 - accuracy: 0.4341 - 41s/epoch - 490ms/step\n",
            "Epoch 8/80\n",
            "84/84 - 37s - loss: 1.8955 - accuracy: 0.4533 - 37s/epoch - 439ms/step\n",
            "Epoch 9/80\n",
            "84/84 - 35s - loss: 1.8383 - accuracy: 0.4686 - 35s/epoch - 416ms/step\n",
            "Epoch 10/80\n",
            "84/84 - 35s - loss: 1.7853 - accuracy: 0.4825 - 35s/epoch - 416ms/step\n",
            "Epoch 11/80\n",
            "84/84 - 35s - loss: 1.7391 - accuracy: 0.4945 - 35s/epoch - 416ms/step\n",
            "Epoch 12/80\n",
            "84/84 - 35s - loss: 1.6935 - accuracy: 0.5065 - 35s/epoch - 412ms/step\n",
            "Epoch 13/80\n",
            "84/84 - 35s - loss: 1.6548 - accuracy: 0.5166 - 35s/epoch - 414ms/step\n",
            "Epoch 14/80\n",
            "84/84 - 35s - loss: 1.6146 - accuracy: 0.5267 - 35s/epoch - 411ms/step\n",
            "Epoch 15/80\n",
            "84/84 - 35s - loss: 1.5796 - accuracy: 0.5366 - 35s/epoch - 413ms/step\n",
            "Epoch 16/80\n",
            "84/84 - 35s - loss: 1.5452 - accuracy: 0.5459 - 35s/epoch - 411ms/step\n",
            "Epoch 17/80\n",
            "84/84 - 35s - loss: 1.5089 - accuracy: 0.5555 - 35s/epoch - 412ms/step\n",
            "Epoch 18/80\n",
            "84/84 - 35s - loss: 1.4795 - accuracy: 0.5624 - 35s/epoch - 411ms/step\n",
            "Epoch 19/80\n",
            "84/84 - 35s - loss: 1.4477 - accuracy: 0.5718 - 35s/epoch - 412ms/step\n",
            "Epoch 20/80\n",
            "84/84 - 35s - loss: 1.4183 - accuracy: 0.5799 - 35s/epoch - 416ms/step\n",
            "Epoch 21/80\n",
            "84/84 - 35s - loss: 1.3859 - accuracy: 0.5895 - 35s/epoch - 411ms/step\n",
            "Epoch 22/80\n",
            "84/84 - 35s - loss: 1.3570 - accuracy: 0.5970 - 35s/epoch - 421ms/step\n",
            "Epoch 23/80\n",
            "84/84 - 35s - loss: 1.3298 - accuracy: 0.6044 - 35s/epoch - 416ms/step\n",
            "Epoch 24/80\n",
            "84/84 - 35s - loss: 1.3013 - accuracy: 0.6126 - 35s/epoch - 412ms/step\n",
            "Epoch 25/80\n",
            "84/84 - 35s - loss: 1.2706 - accuracy: 0.6213 - 35s/epoch - 420ms/step\n",
            "Epoch 26/80\n",
            "84/84 - 36s - loss: 1.2454 - accuracy: 0.6284 - 36s/epoch - 426ms/step\n",
            "Epoch 27/80\n",
            "84/84 - 36s - loss: 1.2166 - accuracy: 0.6365 - 36s/epoch - 424ms/step\n",
            "Epoch 28/80\n",
            "84/84 - 36s - loss: 1.1899 - accuracy: 0.6447 - 36s/epoch - 428ms/step\n",
            "Epoch 29/80\n",
            "84/84 - 36s - loss: 1.1610 - accuracy: 0.6531 - 36s/epoch - 425ms/step\n",
            "Epoch 30/80\n",
            "84/84 - 36s - loss: 1.1375 - accuracy: 0.6603 - 36s/epoch - 426ms/step\n",
            "Epoch 31/80\n",
            "84/84 - 36s - loss: 1.1052 - accuracy: 0.6703 - 36s/epoch - 431ms/step\n",
            "Epoch 32/80\n",
            "84/84 - 36s - loss: 1.0763 - accuracy: 0.6785 - 36s/epoch - 429ms/step\n",
            "Epoch 33/80\n",
            "84/84 - 36s - loss: 1.0510 - accuracy: 0.6862 - 36s/epoch - 426ms/step\n",
            "Epoch 34/80\n",
            "84/84 - 36s - loss: 1.0292 - accuracy: 0.6925 - 36s/epoch - 424ms/step\n",
            "Epoch 35/80\n",
            "84/84 - 36s - loss: 0.9988 - accuracy: 0.7017 - 36s/epoch - 427ms/step\n",
            "Epoch 36/80\n",
            "84/84 - 36s - loss: 0.9669 - accuracy: 0.7118 - 36s/epoch - 428ms/step\n",
            "Epoch 37/80\n",
            "84/84 - 36s - loss: 0.9424 - accuracy: 0.7188 - 36s/epoch - 423ms/step\n",
            "Epoch 38/80\n",
            "84/84 - 36s - loss: 0.9172 - accuracy: 0.7260 - 36s/epoch - 424ms/step\n",
            "Epoch 39/80\n",
            "84/84 - 36s - loss: 0.8843 - accuracy: 0.7366 - 36s/epoch - 427ms/step\n",
            "Epoch 40/80\n",
            "84/84 - 36s - loss: 0.8728 - accuracy: 0.7393 - 36s/epoch - 429ms/step\n",
            "Epoch 41/80\n",
            "84/84 - 36s - loss: 0.8370 - accuracy: 0.7522 - 36s/epoch - 427ms/step\n",
            "Epoch 42/80\n",
            "84/84 - 36s - loss: 0.8177 - accuracy: 0.7568 - 36s/epoch - 426ms/step\n",
            "Epoch 43/80\n",
            "84/84 - 36s - loss: 0.7814 - accuracy: 0.7690 - 36s/epoch - 426ms/step\n",
            "Epoch 44/80\n",
            "84/84 - 36s - loss: 0.7654 - accuracy: 0.7740 - 36s/epoch - 424ms/step\n",
            "Epoch 45/80\n",
            "84/84 - 36s - loss: 0.7335 - accuracy: 0.7851 - 36s/epoch - 424ms/step\n",
            "Epoch 46/80\n",
            "84/84 - 35s - loss: 0.7086 - accuracy: 0.7933 - 35s/epoch - 420ms/step\n",
            "Epoch 47/80\n",
            "84/84 - 36s - loss: 0.6969 - accuracy: 0.7945 - 36s/epoch - 423ms/step\n",
            "Epoch 48/80\n",
            "84/84 - 35s - loss: 0.6600 - accuracy: 0.8080 - 35s/epoch - 421ms/step\n",
            "Epoch 49/80\n",
            "84/84 - 36s - loss: 0.6424 - accuracy: 0.8132 - 36s/epoch - 424ms/step\n",
            "Epoch 50/80\n",
            "84/84 - 36s - loss: 0.6263 - accuracy: 0.8179 - 36s/epoch - 433ms/step\n",
            "Epoch 51/80\n",
            "84/84 - 36s - loss: 0.6039 - accuracy: 0.8250 - 36s/epoch - 427ms/step\n",
            "Epoch 52/80\n",
            "84/84 - 36s - loss: 0.5838 - accuracy: 0.8310 - 36s/epoch - 428ms/step\n",
            "Epoch 53/80\n",
            "84/84 - 36s - loss: 0.5491 - accuracy: 0.8445 - 36s/epoch - 427ms/step\n",
            "Epoch 54/80\n",
            "84/84 - 36s - loss: 0.5429 - accuracy: 0.8442 - 36s/epoch - 426ms/step\n",
            "Epoch 55/80\n",
            "84/84 - 36s - loss: 0.5114 - accuracy: 0.8557 - 36s/epoch - 425ms/step\n",
            "Epoch 56/80\n",
            "84/84 - 35s - loss: 0.5017 - accuracy: 0.8579 - 35s/epoch - 411ms/step\n",
            "Epoch 57/80\n",
            "84/84 - 35s - loss: 0.4776 - accuracy: 0.8669 - 35s/epoch - 422ms/step\n",
            "Epoch 58/80\n",
            "84/84 - 36s - loss: 0.4662 - accuracy: 0.8691 - 36s/epoch - 430ms/step\n",
            "Epoch 59/80\n",
            "84/84 - 35s - loss: 0.4568 - accuracy: 0.8716 - 35s/epoch - 422ms/step\n",
            "Epoch 60/80\n",
            "84/84 - 36s - loss: 0.4195 - accuracy: 0.8859 - 36s/epoch - 429ms/step\n",
            "Epoch 61/80\n",
            "84/84 - 36s - loss: 0.3955 - accuracy: 0.8945 - 36s/epoch - 430ms/step\n",
            "Epoch 62/80\n",
            "84/84 - 36s - loss: 0.3865 - accuracy: 0.8962 - 36s/epoch - 429ms/step\n",
            "Epoch 63/80\n",
            "84/84 - 36s - loss: 0.3721 - accuracy: 0.9008 - 36s/epoch - 431ms/step\n",
            "Epoch 64/80\n",
            "84/84 - 36s - loss: 0.3903 - accuracy: 0.8914 - 36s/epoch - 431ms/step\n",
            "Epoch 65/80\n",
            "84/84 - 36s - loss: 0.3527 - accuracy: 0.9054 - 36s/epoch - 431ms/step\n",
            "Epoch 66/80\n",
            "84/84 - 36s - loss: 0.3302 - accuracy: 0.9139 - 36s/epoch - 426ms/step\n",
            "Epoch 67/80\n",
            "84/84 - 36s - loss: 0.3194 - accuracy: 0.9169 - 36s/epoch - 427ms/step\n",
            "Epoch 68/80\n",
            "84/84 - 36s - loss: 0.3119 - accuracy: 0.9190 - 36s/epoch - 433ms/step\n",
            "Epoch 69/80\n",
            "84/84 - 36s - loss: 0.3029 - accuracy: 0.9216 - 36s/epoch - 433ms/step\n",
            "Epoch 70/80\n",
            "84/84 - 37s - loss: 0.2785 - accuracy: 0.9303 - 37s/epoch - 438ms/step\n",
            "Epoch 71/80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, length):\n",
        "  #문자에 대한 랜덤한 정수 생성\n",
        "  ix = [np.random.randint(vocab_size)]\n",
        "\n",
        "  #랜덤한 정수로부터 맵핑되는 문자 생성\n",
        "  y_char = [index_to_char[ix[-1]]]\n",
        "  print(ix[-1], '번 문자', y_char[-1], '로 예측을 시작!')\n",
        "\n",
        "  #(1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
        "  X = np.zeros((1, length, vocab_size))\n",
        "\n",
        "  for i in range(length):\n",
        "    # X[0][i][예측한 문자의 인덱스] = 1, 즉, 예측 문자를 다음 입력 시퀀스에 추가\n",
        "    X[0][i][ix[-1]] = 1\n",
        "    print(index_to_char[ix[-1]], end=\"\")\n",
        "    ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
        "    y_char.append(index_to_char[ix[-1]])\n",
        "\n",
        "  return ('').join(y_char)"
      ],
      "metadata": {
        "id": "m7MlsqcqRdBu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = sentence_generation(model, 100)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4D8PSW7ShZ6",
        "outputId": "75dae54c-54ed-4f10-823c-071888ee257e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35 번 문자 f 로 예측을 시작!\n",
            "f the room. the cook threw a frying-pan after her as she went on and how do you know that it might hf the room. the cook threw a frying-pan after her as she went on and how do you know that it might ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자단위 RNN(Char RNN)으로 텍스트 생성하기\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "RTdf5yU1Smdr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = '''\n",
        "I get on with life as a programmer,\n",
        "I like to contemplate beer.\n",
        "But when I start to daydream,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "Do I love wine more than beer?\n",
        "\n",
        "I like to use words about beer.\n",
        "But when I stop my talking,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "I hate bugs and errors.\n",
        "But I just think back to wine,\n",
        "And I'm happy once again.\n",
        "\n",
        "I like to hang out with programming and deep learning.\n",
        "But when left alone,\n",
        "My mind turns straight to wine.\n",
        "'''"
      ],
      "metadata": {
        "id": "x_dtAfEMy7R5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "print(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z7NTC3S1rqq",
        "outputId": "d94c33ee-ca3e-4805-849a-ecc1b26eab25"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#중복을 제거한 문자 집합 생성\n",
        "char_vocab = sorted(list(set(raw_text)))\n",
        "vocab_size = len(char_vocab)\n",
        "print('문자 집합 : ', char_vocab)\n",
        "print('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACFRQOZc1yfE",
        "outputId": "7aeb15c1-5cd6-4db0-e7ee-3fd6f81cd483"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 :  [' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
            "문자 집합의 크기 : 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((char, index) for index, char in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWNtSRSw2EsR",
        "outputId": "e9199f16-fb86-45d2-bc00-5459e85688d3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length = 11\n",
        "sequences = []\n",
        "for i in range(length, len(raw_text)):\n",
        "  seq = raw_text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.\n",
        "  sequences.append(seq)\n",
        "\n",
        "print('총 훈련 샘플의 수 : %d' % len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgbwG_162RJD",
        "outputId": "3a09f9be-9214-4aa3-d423-6d23b0975818"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 훈련 샘플의 수 : 426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmYmle5_2oi5",
        "outputId": "5b3ca9f4-66ac-45b2-df1d-5361b7392f5d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I get on wi',\n",
              " ' get on wit',\n",
              " 'get on with',\n",
              " 'et on with ',\n",
              " 't on with l',\n",
              " ' on with li',\n",
              " 'on with lif',\n",
              " 'n with life',\n",
              " ' with life ',\n",
              " 'with life a']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = []\n",
        "for sequence in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.\n",
        "  encoded_sequence = [char_to_index[char] for char in sequence] #문장 샘플에서 각 문자에 대해서 정수 인코딩을 수행.\n",
        "  encoded_sequences.append(encoded_sequence)"
      ],
      "metadata": {
        "id": "il9IbzPa2qjt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDP5jXhm3Jpr",
        "outputId": "cd2ac07e-3ab9-4b42-f637-1abbf3f790f9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18],\n",
              " [0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28],\n",
              " [16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17],\n",
              " [14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0],\n",
              " [28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = np.array(encoded_sequences)\n",
        "\n",
        "#맨 마지막 위치의 문자를 분리\n",
        "X_data = encoded_sequences[:, :-1]\n",
        "\n",
        "#맨 마지막 위치의 문자를 저장\n",
        "y_data = encoded_sequences[:, -1]"
      ],
      "metadata": {
        "id": "1ZYDFoSo3LP6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data[:5])\n",
        "print('-'*40)\n",
        "print(y_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gIzaIRs3bqp",
        "outputId": "ce27e6d4-375d-400b-e3be-750d0ec0a6cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8  0 16 14 28  0 24 23  0 31]\n",
            " [ 0 16 14 28  0 24 23  0 31 18]\n",
            " [16 14 28  0 24 23  0 31 18 28]\n",
            " [14 28  0 24 23  0 31 18 28 17]\n",
            " [28  0 24 23  0 31 18 28 17  0]]\n",
            "----------------------------------------\n",
            "[18 28 17  0 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#원-핫 인코딩\n",
        "X_data_one_hot = [to_categorical(encoded, num_classes=vocab_size) for encoded in X_data]\n",
        "X_data_one_hot = np.array(X_data_one_hot)\n",
        "y_data_one_hot = to_categorical(y_data, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "CyP8n0k13foq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvLLRiCc3o1-",
        "outputId": "29930165-cb26-4a18-d198-81caa908f377"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(426, 10, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 설계\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "hidden_units = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_data_one_hot, y_data_one_hot, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZeqXEEd4ABn",
        "outputId": "343409b0-9b14-4fc5-8d34-4f39c46cc7bd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "14/14 - 2s - loss: 3.4684 - accuracy: 0.0892 - 2s/epoch - 129ms/step\n",
            "Epoch 2/100\n",
            "14/14 - 0s - loss: 3.3724 - accuracy: 0.2136 - 78ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "14/14 - 0s - loss: 3.1120 - accuracy: 0.1972 - 74ms/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "14/14 - 0s - loss: 2.9910 - accuracy: 0.1972 - 80ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "14/14 - 0s - loss: 2.9504 - accuracy: 0.1972 - 117ms/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "14/14 - 0s - loss: 2.9298 - accuracy: 0.1972 - 123ms/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "14/14 - 0s - loss: 2.9083 - accuracy: 0.1972 - 76ms/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "14/14 - 0s - loss: 2.8853 - accuracy: 0.1972 - 72ms/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "14/14 - 0s - loss: 2.8648 - accuracy: 0.1972 - 73ms/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "14/14 - 0s - loss: 2.8423 - accuracy: 0.1972 - 77ms/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "14/14 - 0s - loss: 2.8087 - accuracy: 0.1972 - 76ms/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "14/14 - 0s - loss: 2.7822 - accuracy: 0.2019 - 75ms/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "14/14 - 0s - loss: 2.7324 - accuracy: 0.2113 - 74ms/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "14/14 - 0s - loss: 2.7026 - accuracy: 0.2136 - 120ms/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "14/14 - 0s - loss: 2.6830 - accuracy: 0.2418 - 97ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "14/14 - 0s - loss: 2.6390 - accuracy: 0.2441 - 84ms/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "14/14 - 0s - loss: 2.6062 - accuracy: 0.2629 - 81ms/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "14/14 - 0s - loss: 2.5652 - accuracy: 0.2535 - 73ms/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "14/14 - 0s - loss: 2.5273 - accuracy: 0.2746 - 76ms/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "14/14 - 0s - loss: 2.4792 - accuracy: 0.3216 - 74ms/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "14/14 - 0s - loss: 2.4341 - accuracy: 0.2981 - 82ms/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "14/14 - 0s - loss: 2.3917 - accuracy: 0.3357 - 77ms/epoch - 5ms/step\n",
            "Epoch 23/100\n",
            "14/14 - 0s - loss: 2.3720 - accuracy: 0.3404 - 73ms/epoch - 5ms/step\n",
            "Epoch 24/100\n",
            "14/14 - 0s - loss: 2.3063 - accuracy: 0.3427 - 74ms/epoch - 5ms/step\n",
            "Epoch 25/100\n",
            "14/14 - 0s - loss: 2.2580 - accuracy: 0.3592 - 80ms/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "14/14 - 0s - loss: 2.2322 - accuracy: 0.3756 - 77ms/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "14/14 - 0s - loss: 2.2049 - accuracy: 0.3732 - 74ms/epoch - 5ms/step\n",
            "Epoch 28/100\n",
            "14/14 - 0s - loss: 2.1408 - accuracy: 0.4085 - 72ms/epoch - 5ms/step\n",
            "Epoch 29/100\n",
            "14/14 - 0s - loss: 2.1147 - accuracy: 0.4155 - 82ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "14/14 - 0s - loss: 2.0657 - accuracy: 0.4202 - 71ms/epoch - 5ms/step\n",
            "Epoch 31/100\n",
            "14/14 - 0s - loss: 2.0076 - accuracy: 0.4343 - 74ms/epoch - 5ms/step\n",
            "Epoch 32/100\n",
            "14/14 - 0s - loss: 1.9656 - accuracy: 0.4390 - 73ms/epoch - 5ms/step\n",
            "Epoch 33/100\n",
            "14/14 - 0s - loss: 1.9272 - accuracy: 0.4531 - 72ms/epoch - 5ms/step\n",
            "Epoch 34/100\n",
            "14/14 - 0s - loss: 1.8973 - accuracy: 0.4742 - 72ms/epoch - 5ms/step\n",
            "Epoch 35/100\n",
            "14/14 - 0s - loss: 1.8589 - accuracy: 0.4765 - 75ms/epoch - 5ms/step\n",
            "Epoch 36/100\n",
            "14/14 - 0s - loss: 1.7978 - accuracy: 0.5141 - 73ms/epoch - 5ms/step\n",
            "Epoch 37/100\n",
            "14/14 - 0s - loss: 1.7809 - accuracy: 0.5117 - 74ms/epoch - 5ms/step\n",
            "Epoch 38/100\n",
            "14/14 - 0s - loss: 1.7164 - accuracy: 0.5329 - 82ms/epoch - 6ms/step\n",
            "Epoch 39/100\n",
            "14/14 - 0s - loss: 1.6779 - accuracy: 0.5423 - 74ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "14/14 - 0s - loss: 1.6414 - accuracy: 0.5563 - 73ms/epoch - 5ms/step\n",
            "Epoch 41/100\n",
            "14/14 - 0s - loss: 1.6238 - accuracy: 0.5634 - 77ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "14/14 - 0s - loss: 1.5812 - accuracy: 0.5892 - 84ms/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "14/14 - 0s - loss: 1.5335 - accuracy: 0.5704 - 75ms/epoch - 5ms/step\n",
            "Epoch 44/100\n",
            "14/14 - 0s - loss: 1.5057 - accuracy: 0.6009 - 76ms/epoch - 5ms/step\n",
            "Epoch 45/100\n",
            "14/14 - 0s - loss: 1.4594 - accuracy: 0.6197 - 72ms/epoch - 5ms/step\n",
            "Epoch 46/100\n",
            "14/14 - 0s - loss: 1.4372 - accuracy: 0.6385 - 73ms/epoch - 5ms/step\n",
            "Epoch 47/100\n",
            "14/14 - 0s - loss: 1.3729 - accuracy: 0.6831 - 98ms/epoch - 7ms/step\n",
            "Epoch 48/100\n",
            "14/14 - 0s - loss: 1.3392 - accuracy: 0.6667 - 80ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "14/14 - 0s - loss: 1.3156 - accuracy: 0.6714 - 72ms/epoch - 5ms/step\n",
            "Epoch 50/100\n",
            "14/14 - 0s - loss: 1.2988 - accuracy: 0.6526 - 78ms/epoch - 6ms/step\n",
            "Epoch 51/100\n",
            "14/14 - 0s - loss: 1.2326 - accuracy: 0.7136 - 74ms/epoch - 5ms/step\n",
            "Epoch 52/100\n",
            "14/14 - 0s - loss: 1.1904 - accuracy: 0.7207 - 86ms/epoch - 6ms/step\n",
            "Epoch 53/100\n",
            "14/14 - 0s - loss: 1.1770 - accuracy: 0.7535 - 73ms/epoch - 5ms/step\n",
            "Epoch 54/100\n",
            "14/14 - 0s - loss: 1.1608 - accuracy: 0.7183 - 82ms/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "14/14 - 0s - loss: 1.1008 - accuracy: 0.7582 - 75ms/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "14/14 - 0s - loss: 1.0790 - accuracy: 0.7676 - 78ms/epoch - 6ms/step\n",
            "Epoch 57/100\n",
            "14/14 - 0s - loss: 1.0400 - accuracy: 0.7582 - 74ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "14/14 - 0s - loss: 1.0305 - accuracy: 0.7817 - 72ms/epoch - 5ms/step\n",
            "Epoch 59/100\n",
            "14/14 - 0s - loss: 0.9968 - accuracy: 0.7653 - 73ms/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "14/14 - 0s - loss: 0.9573 - accuracy: 0.7934 - 81ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "14/14 - 0s - loss: 0.9320 - accuracy: 0.8052 - 77ms/epoch - 5ms/step\n",
            "Epoch 62/100\n",
            "14/14 - 0s - loss: 0.9421 - accuracy: 0.7958 - 75ms/epoch - 5ms/step\n",
            "Epoch 63/100\n",
            "14/14 - 0s - loss: 0.8910 - accuracy: 0.8146 - 76ms/epoch - 5ms/step\n",
            "Epoch 64/100\n",
            "14/14 - 0s - loss: 0.8573 - accuracy: 0.8263 - 76ms/epoch - 5ms/step\n",
            "Epoch 65/100\n",
            "14/14 - 0s - loss: 0.8294 - accuracy: 0.8568 - 80ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "14/14 - 0s - loss: 0.7900 - accuracy: 0.8545 - 77ms/epoch - 5ms/step\n",
            "Epoch 67/100\n",
            "14/14 - 0s - loss: 0.7864 - accuracy: 0.8427 - 231ms/epoch - 16ms/step\n",
            "Epoch 68/100\n",
            "14/14 - 0s - loss: 0.7578 - accuracy: 0.8638 - 79ms/epoch - 6ms/step\n",
            "Epoch 69/100\n",
            "14/14 - 0s - loss: 0.7351 - accuracy: 0.8545 - 72ms/epoch - 5ms/step\n",
            "Epoch 70/100\n",
            "14/14 - 0s - loss: 0.7102 - accuracy: 0.8803 - 74ms/epoch - 5ms/step\n",
            "Epoch 71/100\n",
            "14/14 - 0s - loss: 0.6809 - accuracy: 0.8803 - 73ms/epoch - 5ms/step\n",
            "Epoch 72/100\n",
            "14/14 - 0s - loss: 0.6680 - accuracy: 0.8850 - 72ms/epoch - 5ms/step\n",
            "Epoch 73/100\n",
            "14/14 - 0s - loss: 0.6407 - accuracy: 0.8826 - 72ms/epoch - 5ms/step\n",
            "Epoch 74/100\n",
            "14/14 - 0s - loss: 0.6204 - accuracy: 0.8897 - 73ms/epoch - 5ms/step\n",
            "Epoch 75/100\n",
            "14/14 - 0s - loss: 0.6063 - accuracy: 0.8873 - 179ms/epoch - 13ms/step\n",
            "Epoch 76/100\n",
            "14/14 - 0s - loss: 0.5872 - accuracy: 0.9108 - 121ms/epoch - 9ms/step\n",
            "Epoch 77/100\n",
            "14/14 - 0s - loss: 0.5630 - accuracy: 0.9178 - 75ms/epoch - 5ms/step\n",
            "Epoch 78/100\n",
            "14/14 - 0s - loss: 0.5437 - accuracy: 0.9178 - 72ms/epoch - 5ms/step\n",
            "Epoch 79/100\n",
            "14/14 - 0s - loss: 0.5299 - accuracy: 0.9249 - 77ms/epoch - 5ms/step\n",
            "Epoch 80/100\n",
            "14/14 - 0s - loss: 0.5183 - accuracy: 0.9366 - 73ms/epoch - 5ms/step\n",
            "Epoch 81/100\n",
            "14/14 - 0s - loss: 0.4949 - accuracy: 0.9319 - 73ms/epoch - 5ms/step\n",
            "Epoch 82/100\n",
            "14/14 - 0s - loss: 0.4891 - accuracy: 0.9319 - 72ms/epoch - 5ms/step\n",
            "Epoch 83/100\n",
            "14/14 - 0s - loss: 0.4654 - accuracy: 0.9366 - 166ms/epoch - 12ms/step\n",
            "Epoch 84/100\n",
            "14/14 - 0s - loss: 0.4572 - accuracy: 0.9390 - 118ms/epoch - 8ms/step\n",
            "Epoch 85/100\n",
            "14/14 - 0s - loss: 0.4493 - accuracy: 0.9249 - 76ms/epoch - 5ms/step\n",
            "Epoch 86/100\n",
            "14/14 - 0s - loss: 0.4417 - accuracy: 0.9413 - 77ms/epoch - 5ms/step\n",
            "Epoch 87/100\n",
            "14/14 - 0s - loss: 0.4203 - accuracy: 0.9413 - 75ms/epoch - 5ms/step\n",
            "Epoch 88/100\n",
            "14/14 - 0s - loss: 0.4206 - accuracy: 0.9366 - 84ms/epoch - 6ms/step\n",
            "Epoch 89/100\n",
            "14/14 - 0s - loss: 0.4168 - accuracy: 0.9437 - 82ms/epoch - 6ms/step\n",
            "Epoch 90/100\n",
            "14/14 - 0s - loss: 0.3874 - accuracy: 0.9460 - 70ms/epoch - 5ms/step\n",
            "Epoch 91/100\n",
            "14/14 - 0s - loss: 0.3676 - accuracy: 0.9507 - 72ms/epoch - 5ms/step\n",
            "Epoch 92/100\n",
            "14/14 - 0s - loss: 0.3625 - accuracy: 0.9507 - 75ms/epoch - 5ms/step\n",
            "Epoch 93/100\n",
            "14/14 - 0s - loss: 0.3486 - accuracy: 0.9577 - 73ms/epoch - 5ms/step\n",
            "Epoch 94/100\n",
            "14/14 - 0s - loss: 0.3323 - accuracy: 0.9648 - 80ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "14/14 - 0s - loss: 0.3290 - accuracy: 0.9648 - 76ms/epoch - 5ms/step\n",
            "Epoch 96/100\n",
            "14/14 - 0s - loss: 0.3134 - accuracy: 0.9671 - 77ms/epoch - 5ms/step\n",
            "Epoch 97/100\n",
            "14/14 - 0s - loss: 0.3068 - accuracy: 0.9765 - 76ms/epoch - 5ms/step\n",
            "Epoch 98/100\n",
            "14/14 - 0s - loss: 0.3024 - accuracy: 0.9695 - 78ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "14/14 - 0s - loss: 0.2906 - accuracy: 0.9742 - 76ms/epoch - 5ms/step\n",
            "Epoch 100/100\n",
            "14/14 - 0s - loss: 0.2825 - accuracy: 0.9718 - 78ms/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e36125150>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, char_to_index, seq_length, seed_text, n):\n",
        "\n",
        "  #초기 시퀀스\n",
        "  init_text = seed_text\n",
        "  sentence = ''\n",
        "\n",
        "  #다음 문자 예측은 총 n번만 반복.\n",
        "  for i in range(n):\n",
        "    encoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩\n",
        "    encoded = pad_sequences([encoded], maxlen = seq_length, padding='pre') # 데이터에 대한 패딩\n",
        "    encoded = to_categorical(encoded, num_classes=len(char_to_index))\n",
        "\n",
        "    #입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 문자)를 result에 저장\n",
        "    result = model.predict(encoded, verbose=0)\n",
        "    result = np.argmax(result, axis = 1)\n",
        "\n",
        "    for char, index in char_to_index.items():\n",
        "      if index == result:\n",
        "        break\n",
        "\n",
        "    #현재 시퀀스 + 예측 문자를 현재 시퀀스로 변경\n",
        "    seed_text = seed_text + char\n",
        "\n",
        "    #예측 문자를 문장에 저장\n",
        "    sentence = sentence + char\n",
        "\n",
        "  \n",
        "  #n번의 다음 문자 예측이 끝나면 최종 완성된 문장을 리턴\n",
        "  sentence = init_text + sentence\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "RI37-B_R43Zv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, char_to_index, 10, 'I get on w', 80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntS-_MG-6GKj",
        "outputId": "12cc8769-3eb4-4817-bdcc-163db544627b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I get on with life as a programmer, I like to hang out with programming and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GNdy4w5z6OAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}